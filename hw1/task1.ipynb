{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Задание 1\n",
    "Сгенерируйте датасет с 10000 наблюдений и 1000 колонок (сэмплируйте из разных распеределений) и сформируйте из него таргет на основе 100 колонок + зашумление (общее или небольшое для каждой колонки - постарайтесь сделать так, чтобы шум не сильно влиял на корреляции между предикторами и таргетами). Удостоверьтесь, что в датасете существуют колонки, которые не использовались для таргета, но при этом имеют высокую корреляцию с теми, что использовались (покажите это в коде)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n0   0.496714   1.280471  -1.122551   0.524982   0.987901   0.017862   \n1  -0.138264  -0.245924   2.796120   0.690287   1.879859   0.255147   \n2   0.647689   1.342503   0.689283   0.484867  -2.813737   0.290908   \n3   1.523030   1.935503  -1.021214  -0.643541   0.897376   0.230489   \n4  -0.234153   0.975839  -1.576624  -0.670365  -0.653806  -0.146138   \n\n   feature_6  feature_7  feature_8  feature_9  ...  feature_990  feature_991  \\\n0   0.312194  -0.609789   1.002612   0.293701  ...     0.043637    -0.010377   \n1  -0.601062   0.838519  -1.380665  -0.638074  ...     1.957549     1.883027   \n2   0.143389  -1.159115  -1.946456  -0.664319  ...     1.227660     1.341263   \n3   1.918386   0.366521   0.354375   0.049559  ...     0.085804     0.072291   \n4   0.832448   0.083562  -0.729070  -0.278974  ...     1.705048     1.732025   \n\n   feature_992  feature_993  feature_994  feature_995  feature_996  \\\n0     0.050621     0.035498    -0.095048     0.021487    -0.041451   \n1     1.991596     1.905200     2.040944     1.970121     2.013584   \n2     1.264885     1.323409     1.300004     1.135428     1.306268   \n3     0.059940    -0.018429    -0.005226     0.017241     0.038870   \n4     1.636691     1.703631     1.737022     1.676540     1.714130   \n\n   feature_997  feature_998  feature_999  \n0     0.060218    -0.128142     0.030686  \n1     1.949059     1.943506     1.907892  \n2     1.252321     1.253547     1.201880  \n3     0.033559     0.113008     0.137452  \n4     1.739646     1.793698     1.780125  \n\n[5 rows x 1000 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>feature_9</th>\n      <th>...</th>\n      <th>feature_990</th>\n      <th>feature_991</th>\n      <th>feature_992</th>\n      <th>feature_993</th>\n      <th>feature_994</th>\n      <th>feature_995</th>\n      <th>feature_996</th>\n      <th>feature_997</th>\n      <th>feature_998</th>\n      <th>feature_999</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.496714</td>\n      <td>1.280471</td>\n      <td>-1.122551</td>\n      <td>0.524982</td>\n      <td>0.987901</td>\n      <td>0.017862</td>\n      <td>0.312194</td>\n      <td>-0.609789</td>\n      <td>1.002612</td>\n      <td>0.293701</td>\n      <td>...</td>\n      <td>0.043637</td>\n      <td>-0.010377</td>\n      <td>0.050621</td>\n      <td>0.035498</td>\n      <td>-0.095048</td>\n      <td>0.021487</td>\n      <td>-0.041451</td>\n      <td>0.060218</td>\n      <td>-0.128142</td>\n      <td>0.030686</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.138264</td>\n      <td>-0.245924</td>\n      <td>2.796120</td>\n      <td>0.690287</td>\n      <td>1.879859</td>\n      <td>0.255147</td>\n      <td>-0.601062</td>\n      <td>0.838519</td>\n      <td>-1.380665</td>\n      <td>-0.638074</td>\n      <td>...</td>\n      <td>1.957549</td>\n      <td>1.883027</td>\n      <td>1.991596</td>\n      <td>1.905200</td>\n      <td>2.040944</td>\n      <td>1.970121</td>\n      <td>2.013584</td>\n      <td>1.949059</td>\n      <td>1.943506</td>\n      <td>1.907892</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.647689</td>\n      <td>1.342503</td>\n      <td>0.689283</td>\n      <td>0.484867</td>\n      <td>-2.813737</td>\n      <td>0.290908</td>\n      <td>0.143389</td>\n      <td>-1.159115</td>\n      <td>-1.946456</td>\n      <td>-0.664319</td>\n      <td>...</td>\n      <td>1.227660</td>\n      <td>1.341263</td>\n      <td>1.264885</td>\n      <td>1.323409</td>\n      <td>1.300004</td>\n      <td>1.135428</td>\n      <td>1.306268</td>\n      <td>1.252321</td>\n      <td>1.253547</td>\n      <td>1.201880</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.523030</td>\n      <td>1.935503</td>\n      <td>-1.021214</td>\n      <td>-0.643541</td>\n      <td>0.897376</td>\n      <td>0.230489</td>\n      <td>1.918386</td>\n      <td>0.366521</td>\n      <td>0.354375</td>\n      <td>0.049559</td>\n      <td>...</td>\n      <td>0.085804</td>\n      <td>0.072291</td>\n      <td>0.059940</td>\n      <td>-0.018429</td>\n      <td>-0.005226</td>\n      <td>0.017241</td>\n      <td>0.038870</td>\n      <td>0.033559</td>\n      <td>0.113008</td>\n      <td>0.137452</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.234153</td>\n      <td>0.975839</td>\n      <td>-1.576624</td>\n      <td>-0.670365</td>\n      <td>-0.653806</td>\n      <td>-0.146138</td>\n      <td>0.832448</td>\n      <td>0.083562</td>\n      <td>-0.729070</td>\n      <td>-0.278974</td>\n      <td>...</td>\n      <td>1.705048</td>\n      <td>1.732025</td>\n      <td>1.636691</td>\n      <td>1.703631</td>\n      <td>1.737022</td>\n      <td>1.676540</td>\n      <td>1.714130</td>\n      <td>1.739646</td>\n      <td>1.793698</td>\n      <td>1.780125</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1000 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 10000\n",
    "n_columns = 1000\n",
    "data = np.zeros((n_samples, n_columns))\n",
    "\n",
    "for i in range(n_columns // 4):\n",
    "    data[:, i] = np.random.normal(loc=0, scale=1, size=n_samples)\n",
    "    data[:, i + n_columns // 4] = np.random.uniform(-1, 1, size=n_samples)\n",
    "    data[:, i + n_columns // 2] = np.random.exponential(scale=1, size=n_samples)\n",
    "    data[:, i + 3 * n_columns // 4] = np.random.poisson(lam=3, size=n_samples)\n",
    "\n",
    "# Хотим, чтобы последние 100 колонок были сильно скоррелированными\n",
    "corr_base = np.random.normal(0, 1, size=n_samples)\n",
    "for i in range(-100, 0):\n",
    "    data[:, i] = corr_base + np.random.normal(0, 0.05, size=n_samples)  # шум\n",
    "\n",
    "df = pd.DataFrame(data, columns=[f\"feature_{i}\" for i in range(n_columns)])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:52:35.877782Z",
     "start_time": "2024-11-24T23:52:35.201788Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Создадим таргет\n",
    "used_columns = np.random.choice(range(n_columns - 100), 100, replace=False)\n",
    "target_noise = np.random.normal(0, 0.1, size=n_samples)\n",
    "target = np.sum(df.iloc[:, used_columns].values, axis=1) + target_noise\n",
    "df[\"target\"] = target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:53:54.558125Z",
     "start_time": "2024-11-24T23:53:54.539791Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_763   -0.011870\n",
      "feature_932   -0.011876\n",
      "feature_296   -0.011886\n",
      "feature_988   -0.011918\n",
      "feature_901   -0.011925\n",
      "                 ...   \n",
      "feature_838   -0.025293\n",
      "feature_418   -0.027312\n",
      "feature_635   -0.027908\n",
      "feature_500   -0.028378\n",
      "feature_579   -0.031655\n",
      "Name: target, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = df.corr()\n",
    "target_correlations = correlation_matrix[\"target\"].sort_values(ascending=False)\n",
    "highly_correlated_unused = target_correlations.index[target_correlations.index.str.contains(\"feature_\")][-100:]\n",
    "correlation_summary = target_correlations[highly_correlated_unused]\n",
    "print(correlation_summary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:54:22.956761Z",
     "start_time": "2024-11-24T23:54:11.879271Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реализуйте forward stage wise регрессию стандартным образом и с помощью QR разложения наиболее быстрым образом (засекайте время для всех опробованных вариантов). Замерьте качество и процент колонок, которые были правильно найдены."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = df.drop(columns=[\"target\"]).values\n",
    "y = df[\"target\"].values\n",
    "\n",
    "n_train = int(0.8 * len(y))\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:54:44.518297Z",
     "start_time": "2024-11-24T23:54:43.915162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def fsw(X, y, max_iter=10000, tol=1e-4, step_size=0.01): # forward stage wise регрессия\n",
    "    n_samples, n_features = X.shape\n",
    "    coefficients = np.zeros(n_features)\n",
    "    residuals = y.copy()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Сосчитаем корреляцию\n",
    "        correlations = X.T @ residuals\n",
    "        max_correlation_index = np.argmax(np.abs(correlations))\n",
    "        sign = np.sign(correlations[max_correlation_index])\n",
    "        coefficients[max_correlation_index] += step_size * sign\n",
    "\n",
    "        residuals = y - X @ coefficients\n",
    "\n",
    "        if np.max(np.abs(correlations)) < tol:\n",
    "            break\n",
    "\n",
    "    return coefficients"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:56:00.356185Z",
     "start_time": "2024-11-24T23:56:00.350258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def fsw_qr(X, y, max_iter=1000, tol=1e-4):  # с помощью qr разложения\n",
    "    n_samples, n_features = X.shape\n",
    "    coefficients = np.zeros(n_features)\n",
    "    residuals = y.copy()\n",
    "    Q, R = np.linalg.qr(X)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        correlations = Q.T @ residuals\n",
    "        max_correlation_index = np.argmax(np.abs(correlations))\n",
    "        step_size = correlations[max_correlation_index] / R[max_correlation_index, max_correlation_index]\n",
    "        coefficients[max_correlation_index] += step_size\n",
    "        residuals = y - X @ coefficients\n",
    "        if np.max(np.abs(correlations)) < tol:\n",
    "            break\n",
    "\n",
    "    return coefficients"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:56:42.622052Z",
     "start_time": "2024-11-24T23:56:42.613885Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "start_standard = time.time()\n",
    "coefficients_standard = fsw(X_train, y_train, max_iter=10000, step_size=0.01)\n",
    "end_standard = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:57:57.717733Z",
     "start_time": "2024-11-24T23:57:00.818Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "start_qr = time.time()\n",
    "coefficients_qr = fsw_qr(X_train, y_train, max_iter=10000)\n",
    "end_qr = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:58:38.380908Z",
     "start_time": "2024-11-24T23:58:11.100479Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "predicted_y_standard = X_test @ coefficients_standard\n",
    "predicted_y_qr = X_test @ coefficients_qr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:59:01.369050Z",
     "start_time": "2024-11-24T23:59:01.341733Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "mse_standard = mean_squared_error(y_test, predicted_y_standard)\n",
    "mse_qr = mean_squared_error(y_test, predicted_y_qr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:59:12.108631Z",
     "start_time": "2024-11-24T23:59:12.102129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "selected_standard = np.where(coefficients_standard != 0)[0]\n",
    "selected_qr = np.where(coefficients_qr != 0)[0]\n",
    "\n",
    "correct_standard = len(set(selected_standard) & set(used_columns)) / len(used_columns) * 100\n",
    "correct_qr = len(set(selected_qr) & set(used_columns)) / len(used_columns) * 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:59:23.804154Z",
     "start_time": "2024-11-24T23:59:23.798076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Method       MSE  Time (seconds)  \\\n",
      "0      Standard Forward Stagewise  1.394428       56.895492   \n",
      "1  QR-Optimized Forward Stagewise  0.011401       27.280895   \n",
      "\n",
      "   Correct Features Found (%)  \n",
      "0                       100.0  \n",
      "1                       100.0  \n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"Method\": [\"Standard Forward Stagewise\", \"QR-Optimized Forward Stagewise\"],\n",
    "    \"MSE\": [mse_standard, mse_qr],\n",
    "    \"Time (seconds)\": [end_standard - start_standard, end_qr - start_qr],\n",
    "    \"Correct Features Found (%)\": [correct_standard, correct_qr],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T23:59:43.651760Z",
     "start_time": "2024-11-24T23:59:43.645191Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Дополнительно: Попробуйте генерировать данные таким образом, чтобы ошибка постепенно ухудшалась. Подсказка: увеличивайте шум, используйте нелинейные функции и комбинации предикторов. Попробуйте оценить bias и variance для forward stage-wise regression."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Функция для генерации данных, учитывая увеличивающийся шум\n",
    "def generate(n_samples=10000, n_features=1000, noise_levels=None):\n",
    "    if noise_levels is None:\n",
    "        noise_levels = [0.1, 0.5, 1.0, 2.0]\n",
    "    data = np.zeros((n_samples, n_features))\n",
    "\n",
    "    for i in range(n_features // 4):\n",
    "        data[:, i] = np.random.normal(0, 1, size=n_samples)\n",
    "        data[:, i + n_features // 4] = np.random.uniform(-1, 1, size=n_samples)\n",
    "        data[:, i + n_features // 2] = np.random.exponential(1, size=n_samples)\n",
    "        data[:, i + 3 * n_features // 4] = np.random.poisson(3, size=n_samples)\n",
    "\n",
    "    # Создаем таргет, учитывая увеличивающийся шум\n",
    "    targets = []\n",
    "    used_columns = np.random.choice(range(n_features - 100), 100, replace=False)\n",
    "    for noise in noise_levels:\n",
    "        target = np.sum(data[:, used_columns], axis=1)  # лк\n",
    "        target += np.sum(np.sin(data[:, used_columns]), axis=1)  # нелинейные преобразования\n",
    "        target += np.random.normal(0, noise, size=n_samples)  # шум\n",
    "        targets.append(target)\n",
    "\n",
    "    return data, targets, used_columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T00:04:05.786577Z",
     "start_time": "2024-11-25T00:04:05.771047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "X, targets, used_columns = generate()\n",
    "\n",
    "X_train, X_test, y_train_list, y_test_list = train_test_split(\n",
    "    X, np.array(targets).T, test_size=0.2, random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T00:04:30.754295Z",
     "start_time": "2024-11-25T00:04:30.454762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "biases = []\n",
    "biases_qr = []\n",
    "variances = []\n",
    "variances_qr = []\n",
    "mse_list = []\n",
    "mse_list_qr = []\n",
    "\n",
    "for i, y_train in enumerate(y_train_list.T):\n",
    "    y_test = y_test_list[:, i]\n",
    "\n",
    "    coefficients = fsw(X_train, y_train, max_iter=10000, step_size=0.01)\n",
    "    coefficients_qr = fsw_qr(X_train, y_train, max_iter=10000)\n",
    "\n",
    "    predictions_train = X_train @ coefficients\n",
    "    predictions_train_qr = X_train @ coefficients_qr\n",
    "\n",
    "    predictions_test = X_test @ coefficients\n",
    "    predictions_test_qr = X_test @ coefficients_qr\n",
    "\n",
    "    bias = np.mean((np.mean(predictions_test) - y_test) ** 2)\n",
    "    bias_qr = np.mean((np.mean(predictions_test_qr) - y_test) ** 2)\n",
    "\n",
    "    variance = np.var(predictions_test)\n",
    "    variance_qr = np.var(predictions_test_qr)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions_test)\n",
    "    mse_qr = mean_squared_error(y_test, predictions_test_qr)\n",
    "\n",
    "    biases.append(bias)\n",
    "    biases_qr.append(bias_qr)\n",
    "\n",
    "    variances.append(variance)\n",
    "    variances_qr.append(variance_qr)\n",
    "\n",
    "    mse_list.append(mse)\n",
    "    mse_list_qr.append(mse_qr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T00:16:34.314744Z",
     "start_time": "2024-11-25T00:10:14.336660Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Method  Noise Level        Bias   Variance  \\\n",
      "0  Standard Forward Stagewise + noise          0.1  176.828816  93.658298   \n",
      "1  Standard Forward Stagewise + noise          0.5  177.071621  93.694333   \n",
      "2  Standard Forward Stagewise + noise          1.0  177.262838  93.877403   \n",
      "3  Standard Forward Stagewise + noise          2.0  180.277090  93.564031   \n",
      "\n",
      "         MSE  \n",
      "0  43.136812  \n",
      "1  43.141017  \n",
      "2  43.982147  \n",
      "3  47.063059  \n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"Method\": \"Standard Forward Stagewise + noise\",\n",
    "    \"Noise Level\": [0.1, 0.5, 1.0, 2.0],\n",
    "    \"Bias\": biases,\n",
    "    \"Variance\": variances,\n",
    "    \"MSE\": mse_list\n",
    "})\n",
    "\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T00:32:06.710872Z",
     "start_time": "2024-11-25T00:32:06.705079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Method  Noise Level        Bias  \\\n",
      "0  QR-Optimized Forward Stagewise + noise          0.1  176.881543   \n",
      "1  QR-Optimized Forward Stagewise + noise          0.5  177.120127   \n",
      "2  QR-Optimized Forward Stagewise + noise          1.0  177.318761   \n",
      "3  QR-Optimized Forward Stagewise + noise          2.0  180.359764   \n",
      "\n",
      "     Variance        MSE  \n",
      "0  172.583974  14.988887  \n",
      "1  172.635323  15.134655  \n",
      "2  173.248405  16.329562  \n",
      "3  172.790497  19.842370  \n"
     ]
    }
   ],
   "source": [
    "results_qr = pd.DataFrame({\n",
    "    \"Method\": \"QR-Optimized Forward Stagewise + noise\",\n",
    "    \"Noise Level\": [0.1, 0.5, 1.0, 2.0],\n",
    "    \"Bias\": biases_qr,\n",
    "    \"Variance\": variances_qr,\n",
    "    \"MSE\": mse_list_qr\n",
    "})\n",
    "\n",
    "print(results_qr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T00:33:00.045953Z",
     "start_time": "2024-11-25T00:33:00.020580Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
